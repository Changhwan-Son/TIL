# 2020.07.13 TIL

## 빅데이터 기반 자바 웹 개발자 과정 6일차

## 범죄데이터 다루기

## 카카오 API 이용하여 검색 데이터 가져오기

```python
import urllib.request 
import json

import pandas as pd
import numpy as np 

# 카카오 Rest Api Key 
KAKAO_API_KEY = '1e478c0df645ac3a7fc01cf659b47fe8' 

# 카카오 API URL
API_URL = 'https://dapi.kakao.com/v2/local/search/keyword.json'

crime_anal_police = pd.read_csv('./data/02. crime_in_Seoul.csv', thousands=',', encoding='euc-kr')
```

## 경찰서가 무슨 구에 있나?

- 중부서 → 중구
- 종로서 → 종로구 ....

이름만으로는 어디에 있는지 잘 모름 - API 이용한 검색을 통해 알아온다. 

### 검색 잘 나오게 하기 위해 경찰서 이름 편집

- 중부서 → 서울중부경찰서
- 종로서 → 서울종로경찰서
- 남대문서 → 서울남대문경찰서

```python
station_name = []

for name in crime_anal_police['관서명']:
	station_name.append('서울{}경찰서'.format(name[:-1]))
	# 관서명에서 '서'빼고 추가하기 위해 name[:-1] 사용
```

### 카카오 api에 검색어를 전달하기 위한 검색어 인코딩 작업

kakao api에 전달 가능한 형태로 문자열 인코딩 하기 

- urllib.parse.quote 함수를 활용한다.

```python
# 인코딩된 문자열로 바뀜
encText = urllib.parse.quote('서울광진경찰서')

# 카카오 API에 검색까지 포함된 요청을 할 수 있는 요청객체 생성
resquest = urllib.request.Request(API_URL+"?query={}".format(encText))

# kakao에 요청 시에 API_KEY를 넣지 않으면 인증이 안된 사용자라는 메세지가 뜬다. (UnAuthorization)
# 요청 시 카카오 API에 나의 인증정보를 헤더에 전달
request.add_header("Authorization", "KakaoAK {}".format(KAKAO_API_KEY))

# 필요 데이터 추출
# 1. 경찰서 주소
# 2. 위, 경도

response_json = json.loads(response.read().decode('utf-8'))
temp_data = response_json['documents'][0]

temp_data
{'address_name': '서울 광진구 구의동 254-32',
 'category_group_code': 'PO3',
 'category_group_name': '공공기관',
 'category_name': '사회,공공기관 > 행정기관 > 경찰서',
 'distance': '',
 'id': '8211607',
 'phone': '02-2285-7602',
 'place_name': '서울광진경찰서',
 'place_url': 'http://place.map.kakao.com/8211607',
 'road_address_name': '서울 광진구 자양로 167',
 'x': '127.08396170505674',
 'y': '37.54292164557882'}
```

## 전체 경찰서에 대한 주소 및 위,경도 모으기

```python
station_address = [] # 경찰서의 주소를 모을 리스트 
station_lat = [] # 위도 -> X
station_lng = [] # 경도 -> Y

for name in station_name:
	encText = urllib.parse.quete(name)
	request = urllib.request.Request(API_URL+"?query={}".format(encText))

	request.add_header("Authorization", "KakaoAK {}".format(KAKAO_API_KEY))
	# 요청 수행 후 카카오 서버로부터 응답 받기
	response = urllib.request.urlopen(request)

	# 응답 코드 받기   OK : 200, 기타 응답 오류 코드를 수신할 수 있다. 
	rescode = response.getcode()

	# rescode가 200이면 성공적으로 응답 받았다. 
	if rescode == 200 :
		# 응답을 받고, 그 응답에 대한 데이터를 보통 body라고 이야기 한다. 
		response_body = response.read()

		# 응답받은 데이터 (response_body)를 utf-8로 인코딩한 후 json 객체로 만들어주면
		tmp = json.loads(response_body.decode('utf-8'))
		tmp_doc = tmp['documents']
		tmp_data = tmp_doc[0]

		address = tmp_data['address_name']
		lng = tmp_data['y']
		lat = tmp_data['x']

		station_address.append(address)
		station_lat.append(lat)
		station_lng.append(lng)

		print("{} ---> {}".format(name, address))
서울중부경찰서 ---> 서울 중구 저동2가 62-1
서울종로경찰서 ---> 서울 종로구 경운동 90-18
서울남대문경찰서 ---> 서울 중구 남대문로5가 567
서울서대문경찰서 ---> 서울 서대문구 미근동 165
서울혜화경찰서 ---> 서울 종로구 인의동 48-57
서울용산경찰서 ---> 서울 용산구 원효로1가 12-12
서울성북경찰서 ---> 서울 성북구 삼선동5가 301
서울동대문경찰서 ---> 서울 동대문구 청량리동 229
서울마포경찰서 ---> 서울 마포구 아현동 618-1
서울영등포경찰서 ---> 서울 영등포구 당산동3가 2-11
서울성동경찰서 ---> 서울 성동구 행당동 192-8
서울동작경찰서 ---> 서울 동작구 노량진동 72-35
서울광진경찰서 ---> 서울 광진구 구의동 254-32
서울서부경찰서 ---> 서울 은평구 녹번동 177-15
서울강북경찰서 ---> 서울 강북구 번동 415-15
서울금천경찰서 ---> 서울 금천구 시흥동 1030
서울중랑경찰서 ---> 서울 중랑구 신내동 810
서울강남경찰서 ---> 서울 강남구 대치동 998
서울관악경찰서 ---> 서울 관악구 봉천동 1695-5
서울강서경찰서 ---> 서울 양천구 신월동 25
서울강동경찰서 ---> 서울 강동구 성내동 541-1
서울종암경찰서 ---> 서울 성북구 종암동 3-1260
서울구로경찰서 ---> 서울 구로구 구로동 436
서울서초경찰서 ---> 서울 서초구 서초동 1726-1
서울양천경찰서 ---> 서울 양천구 신정동 321
서울송파경찰서 ---> 서울 송파구 가락동 9
서울노원경찰서 ---> 서울 노원구 하계동 250
서울방배경찰서 ---> 서울 서초구 방배동 455-10
서울은평경찰서 ---> 서울 은평구 불광동 산 24
서울도봉경찰서 ---> 서울 도봉구 창동 17
서울수서경찰서 ---> 서울 강남구 개포동 14
```

## 경찰서 위치에 대한 구별 데이터 추가

- split() 으로 문자열을 자르고 인덱스 1번을 ~구로 삼게 되면
    - 서울특별시 ~구 ( O )
    - 서울 ~구 ( O )
    - 서울 특별시 ~구 ( X )
- split('구')로 자르고 난 다음 편집
    - 구의 이름만 등장한다 → 서울 광진구 → [서울 광진 , 가] → split() → [-1] 쓰면 된다.
    - 문제 발생 상황 : 구로구 ( X )

문자열 데이터에서 내가 필요한 정보만 가지고 올 수 있게 하는 것 → 텍스트 마이닝

우리가 하고 있는건 전처리 알고리즘을 직접 생각하고 나서 할 수 있는 텍스트 마이닝 

```python
gu_name = [] 

for name in station_address:
	tmp = name.split() # 공백을 기준으로 자른다.

	# 구로 끝나는 부분만 추출할 수 있는 정규식을 만들어서 사용할 수도 있다. 
	tmp_gu = [gu for gu in tmp if gu[-1] == '구']
	# 공백을 기준으로 잘린 리스트를 검사해서 끝이 '구'로 끝나는 부분만 리스트에 추가

	gu_name.append(tmp_gu[0]) # 구의 이름 추가 
중구
종로구
중구
서대문구
종로구
용산구
성북구
동대문구
마포구
영등포구
성동구
동작구
광진구
은평구
강북구
금천구
중랑구
강남구
관악구
양천구
강동구
성북구
구로구
서초구
양천구
송파구
노원구
서초구
은평구
도봉구
강남구
```

### 각 경찰서에 대한 구의 추출 후 데이터 프레임에 추가

```python
crime_anal_police['구별'] = gu_name
```

## 메인 주제 : 구별 범죄 발생, 범죄 검거에 대한 분석

```python
# 구별 데이터가 잘 수집 되었는지 확인이 필요 
crime_anal_police['구별'].unique()
array(['중구', '종로구', '서대문구', '용산구', '성북구', '동대문구', '마포구', '영등포구', '성동구',
       '동작구', '광진구', '은평구', '강북구', '금천구', '중랑구', '강남구', '관악구', '양천구',
       '강동구', '구로구', '서초구', '송파구', '노원구', '도봉구'], dtype=object)

len(crime_anal_police['구별'].unique()) # 서울시의 구는 25개 행정구가 존재하는데 실제 수집된건 24개
24
```

데이터 분석에서는 내가 분석하고자 하는 대상의 비즈니스를 잘 알고 있는 것도 중요하다. 

- 데이터 분석 전 자료조사가 필요할 수도 있다.
- 설문조사, 논문, 검색 등을 참고한다.

### 데이터 분석에서 제일 시간이 많이 걸리는 것 중에 하나가 비즈니스 분석

```python
# 20200713 기준 서울강서경찰서는 양천구에 임시청사가 있다. 
# 강서구로 변경 
crime_anal_police.loc[crime_anal_police['관서명'] == '강서서', '구별'] = '강서구'
```

## 경찰서가 31개, 서울시의 구는 25개

구별로 분석을 해야함 ⇒ 구별로 집계가 필요 

```python
crime_anal_raw = crime_anal_police.copy() 
# 원본 해치지 않기 위해 copy() 사용 
```

---

# 피벗테이블 개념 잡기

## 피벗테이블 (pivot table)

원본 데이터 프레임에서 특정 기준에 맞게 집계하는 것을 의미한다. 

- ex) 구별 범죄 합계를 구하기
- ex) 구별 검거 평균율 구하기 등...

~ 별로 집계(평균, 합계, 표준편차 등...)을 해야한다면 pivot table을 떠올려 볼 수 있다. 

```python
import pandas as pd 
import numpy as np

df = pd.read_excel('/data/02. sales-funnel.xlsx')
df

Account	Name	Rep	Manager	Product	Quantity	Price	Status
0	714466	Trantow-Barrows	Craig Booker	Debra Henley	CPU	1	30000	presented
1	714466	Trantow-Barrows	Craig Booker	Debra Henley	Software	1	10000	presented
2	714466	Trantow-Barrows	Craig Booker	Debra Henley	Maintenance	2	5000	pending
3	737550	Fritsch, Russel and Anderson	Craig Booker	Debra Henley	CPU	1	35000	declined
4	146832	Kiehn-Spinka	Daniel Hilton	Debra Henley	CPU	2	65000	won
5	218895	Kulas Inc	Daniel Hilton	Debra Henley	CPU	2	40000	pending
6	218895	Kulas Inc	Daniel Hilton	Debra Henley	Software	1	10000	presented
7	412290	Jerde-Hilpert	John Smith	Debra Henley	Maintenance	2	5000	pending
8	740150	Barton LLC	John Smith	Debra Henley	CPU	1	35000	declined
9	141962	Herman LLC	Cedric Moss	Fred Anderson	CPU	2	65000	won
10	163416	Purdy-Kunde	Cedric Moss	Fred Anderson	CPU	1	30000	presented
11	239344	Stokes LLC	Cedric Moss	Fred Anderson	Maintenance	1	5000	pending
12	239344	Stokes LLC	Cedric Moss	Fred Anderson	Software	1	10000	presented
13	307599	Kassulke, Ondricka and Metz	Wendy Yule	Fred Anderson	Maintenance	3	7000	won
14	688981	Keeling LLC	Wendy Yule	Fred Anderson	CPU	5	100000	won
15	729833	Koepp Ltd	Wendy Yule	Fred Anderson	CPU	2	65000	declined
16	729833	Koepp Ltd	Wendy Yule	Fred Anderson	Monitor	2	5000	presented
```

가계별 집계를 하겠다. → 기본 집계는 무조건 평균 → 가계별 평균 집계를 하겠다. 

- 집계 pivot table은 숫자형 데이터에서만 집계가 가능하다.
- 딱히 집계 대상을 지정하지 않는 이상 모든 데이터에 대한 집계가 수행된다.

```python
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 17 entries, 0 to 16
Data columns (total 8 columns):
 #   Column    Non-Null Count  Dtype 
---  ------    --------------  ----- 
 0   Account   17 non-null     int64 
 1   Name      17 non-null     object
 2   Rep       17 non-null     object
 3   Manager   17 non-null     object
 4   Product   17 non-null     object
 5   Quantity  17 non-null     int64 
 6   Price     17 non-null     int64 
 7   Status    17 non-null     object
dtypes: int64(3), object(5)
memory usage: 1.2+ KB

pd.pivot_table(df, index=['Name']) 
# index : 집계의 기준, 딱히 집계 대상을 지정하지 않으면 모든 숫자형(int, float) 데이터에 대한 평균집계

             Account	Price	Quantity
Name			
Barton LLC	740150	35000	1.000000
Fritsch, Russel and Anderson	737550	35000	1.000000
Herman LLC	141962	65000	2.000000
Jerde-Hilpert	412290	5000	2.000000
Kassulke, Ondricka and Metz	307599	7000	3.000000
Keeling LLC	688981	100000	5.000000
Kiehn-Spinka	146832	65000	2.000000
Koepp Ltd	729833	35000	2.000000
Kulas Inc	218895	25000	1.500000
Purdy-Kunde	163416	30000	1.000000
Stokes LLC	239344	7500	1.000000
Trantow-Barrows	714466	15000	1.333333
```

집계에 대한 기준을 여러개 정할 수 있다. 

```python
pd.pivot_table(df, index=['Name', 'Rep', 'Manager']) # group by Name, Rep, Manager

                     Account	Price	Quantity
Name	Rep	Manager			
Barton LLC	John Smith	Debra Henley	740150	35000	1.000000
Fritsch, Russel and Anderson	Craig Booker	Debra Henley	737550	35000	1.000000
Herman LLC	Cedric Moss	Fred Anderson	141962	65000	2.000000
Jerde-Hilpert	John Smith	Debra Henley	412290	5000	2.000000
Kassulke, Ondricka and Metz	Wendy Yule	Fred Anderson	307599	7000	3.000000
Keeling LLC	Wendy Yule	Fred Anderson	688981	100000	5.000000
Kiehn-Spinka	Daniel Hilton	Debra Henley	146832	65000	2.000000
Koepp Ltd	Wendy Yule	Fred Anderson	729833	35000	2.000000
Kulas Inc	Daniel Hilton	Debra Henley	218895	25000	1.500000
Purdy-Kunde	Cedric Moss	Fred Anderson	163416	30000	1.000000
Stokes LLC	Cedric Moss	Fred Anderson	239344	7500	1.000000
Trantow-Barrows	Craig Booker	Debra Henley	714466	15000	1.333333
```

매너지와 영업사원 별 가격 평균 구하기 

```python
pd.pivot_table(df, index=['Manager', 'Rep'], values=['Price'])

                      Price
Manager	Rep	
Debra Henley	Craig Booker	20000.000000
Daniel Hilton	38333.333333
John Smith	20000.000000
Fred Anderson	Cedric Moss	27500.000000
Wendy Yule	44250.000000

pd.pivot_table(df, index=['Name','Product'], values=['Price', 'Quantity'])
                    Price	Quantity
Name	Product		
Barton LLC	CPU	35000	1
Fritsch, Russel and Anderson	CPU	35000	1
Herman LLC	CPU	65000	2
Jerde-Hilpert	Maintenance	5000	2
Kassulke, Ondricka and Metz	Maintenance	7000	3
Keeling LLC	CPU	100000	5
Kiehn-Spinka	CPU	65000	2
Koepp Ltd	CPU	65000	2
Monitor	5000	2
Kulas Inc	CPU	40000	2
Software	10000	1
Purdy-Kunde	CPU	30000	1
Stokes LLC	Maintenance	5000	1
Software	10000	1
Trantow-Barrows	CPU	30000	1
Maintenance	5000	2
Software	10000	1
```

pivot_table에 aggrunc 매개변수를 활용하여 여러 집계를 할 수도 있다. 

```python
pd.pivot_table(df, index=['Manager', 'Rep'], values=['Price'], aggfunc=np.sum)

				       Price
Manager	Rep	
Debra Henley	Craig Booker	80000
Daniel Hilton	115000
John Smith	40000
Fred Anderson	Cedric Moss	110000
Wendy Yule	177000

pd.pivot_table(df, index=['Name', 'Product'], values=['Price', 'Quantity'], aggfunc=[np.sum, np.mean])
												sum	             mean
												Price	Quantity	 Price	Quantity
Name	Product				
Barton LLC	CPU	35000	1	35000	1
Fritsch, Russel and Anderson	CPU	35000	1	35000	1
Herman LLC	CPU	65000	2	65000	2
Jerde-Hilpert	Maintenance	5000	2	5000	2
Kassulke, Ondricka and Metz	Maintenance	7000	3	7000	3
Keeling LLC	CPU	100000	5	100000	5
Kiehn-Spinka	CPU	65000	2	65000	2
Koepp Ltd	CPU	65000	2	65000	2
Monitor	5000	2	5000	2
Kulas Inc	CPU	40000	2	40000	2
Software	10000	1	10000	1
Purdy-Kunde	CPU	30000	1	30000	1
Stokes LLC	Maintenance	5000	1	5000	1
Software	10000	1	10000	1
Trantow-Barrows	CPU	30000	1	30000	1
Maintenance	5000	2	5000	2
Software	10000	1	10000	1

```

피벗테이블은 데이터 프레임에서 ~별 집계를 수행해야 할 때 사용하면 된다. 

- 구별 절도 발생 평균 구하기
- 구별 살인 검거 합계 구하기 등등....

---

## 구별 범죄 발생 횟수의 합, 범죄 검거 횟수의 합

```python
crime_anal = pd.pivot_table(crime_anal_raw, index=['구별'], aggfunc=np.sum)

crime_anal['강간검거율'] = crime_anal['강간 검거'] / crime_anal['강간 발생'] * 100
crime_anal['강도검거율'] = crime_anal['강도 검거'] / crime_anal['강도 발생'] * 100
crime_anal['살인검거율'] = crime_anal['살인 검거'] / crime_anal['살인 발생'] * 100
crime_anal['절도검거율'] = crime_anal['절도 검거'] / crime_anal['절도 발생'] * 100
crime_anal['폭력검거율'] = crime_anal['폭력 검거'] / crime_anal['폭력 발생'] * 100

# 검거율을 사용할 것이기 때문에 검거량에 대한 데이터는 삭제
del crime_anal['강간 검거']
del crime_anal['강도 검거']
del crime_anal['살인 검거']
del crime_anal['절도 검거']
del crime_anal['폭력 검거']
```

검거율이 넘어가는 곳은 전년도 범죄를 검거한 경우 100이 넘어갈 수 있다. 

```python
con_list = ['강간검거율','강도검거율','살인검거율','절도검거율','폭력검거율']

# 각 검거율 컬럼에 대해 100이 넘어가는 값은 100으로 통일 
for column in con_list:
    crime_anal.loc[crime_anal[column] > 100, column] = 100

crime_anal.rename(columns = {
	'강간 발생' : '강간',
	'강도 발생' : '강도',
	'살인 발생' : '살인',
	'절도 발생' : '절도',
	'폭력 발생' : '폭력'}, inplace=True)

```

## 데이터 정규화 (normalize)

- 정규화의 종류
    - 통계적인 데이터 표현을 위해 정규화
    - MinMax 정규화 : 데이터의 편차를 없애기 위해서 사용. 분석하고자 하는 대상의 데이터들의 편차가 클 때 사용
    - Standard 정규화 : 데이터들의 평균을 0으로, 분산을 1, 데이터의 분포를 일정하게 할 때 사용
    - Robust 정규화 : 이상치를 제거하기 위한 목적으로 사용
    - 벡터 정규화 : 데이터의 방향성이 중요할 때 사용
- 정규화의 목적
    - 컴퓨터든, 사람이든 데이터를 놓고 봤을 때 이해 하기 쉬워지도록 데이터를 편집

### sklearn을 활용해 정규화하기

```python
from sklearn import preprocessing

# 정규화 대상 컬럼 선정
cols = ['강간', '강도', '살인', '절도', '폭력']

# 정규화 진행시킬 컬럼, 데이터 가져오기 
x = crime_anal[cols].values
x
array([[ 449,   21,   13, 3850, 4284],
       [ 156,    6,    4, 2366, 2712],
       [ 153,   14,    7, 1434, 2649],
       [ 320,   12,    9, 2706, 3298],
       [ 240,   14,    4, 3026, 2625],
       [ 281,   15,    8, 2335, 3007],
       [ 151,    6,    3, 1567, 2054],
       [ 262,   13,    7, 2096, 3207],
       [ 197,    7,   10, 2193, 2723],
       [ 102,    9,    3, 1063, 1487],
       [ 173,   13,    5, 1981, 2548],
       [ 285,    9,    5, 1865, 1910],
       [ 294,   14,    8, 2555, 2983],
       [ 154,    5,    2, 1812, 2056],
       [ 393,    9,    8, 2635, 2399],
       [ 126,    9,    4, 1607, 1612],
       [ 150,    5,    5, 1785, 2209],
       [ 220,   13,   11, 3239, 3295],
       [ 120,    6,    3, 1890, 2509],
       [ 295,   22,   14, 2964, 3572],
       [ 194,   14,    5, 1557, 2050],
       [ 166,    9,    3, 1914, 2653],
       [ 211,   11,    6, 2184, 2293],
       [ 170,    9,    3, 2548, 2224],
       [ 187,   11,   13, 2135, 2847]])

# MinMaxScaler 가져오기
min_max_scaler = preprocessing.MinMaxScaler()

# 정규화 진행
x_scaled = min_max_scaler.fit_transform(x.astype(float))
# 2차원 배열 x에 들어가 있는 값들을 실수형으로 바꾼 다음 스케일링 진행 
x_scaled

array([[1.        , 0.94117647, 0.91666667, 1.        , 1.        ],
       [0.1556196 , 0.05882353, 0.16666667, 0.46752781, 0.43796925],
       [0.14697406, 0.52941176, 0.41666667, 0.13311805, 0.41544512],
       [0.62824207, 0.41176471, 0.58333333, 0.58952278, 0.64747944],
       [0.39769452, 0.52941176, 0.16666667, 0.70434159, 0.4068645 ],
       [0.51585014, 0.58823529, 0.5       , 0.45640474, 0.5434394 ],
       [0.14121037, 0.05882353, 0.08333333, 0.18083961, 0.2027172 ],
       [0.4610951 , 0.47058824, 0.41666667, 0.37064944, 0.61494458],
       [0.27377522, 0.11764706, 0.66666667, 0.40545389, 0.44190204],
       [0.        , 0.23529412, 0.08333333, 0.        , 0.        ],
       [0.20461095, 0.47058824, 0.25      , 0.32938644, 0.379335  ],
       [0.52737752, 0.23529412, 0.25      , 0.28776462, 0.15123346],
       [0.55331412, 0.52941176, 0.5       , 0.53534266, 0.53485878],
       [0.14985591, 0.        , 0.        , 0.26874776, 0.20343225],
       [0.83861671, 0.23529412, 0.5       , 0.56404736, 0.32606364],
       [0.06916427, 0.23529412, 0.16666667, 0.19519196, 0.04469074],
       [0.13832853, 0.        , 0.25      , 0.25905992, 0.25813371],
       [0.34005764, 0.47058824, 0.75      , 0.78076785, 0.64640686],
       [0.0518732 , 0.05882353, 0.08333333, 0.29673484, 0.36539149],
       [0.55619597, 1.        , 1.        , 0.68209544, 0.74544154],
       [0.26512968, 0.52941176, 0.25      , 0.17725152, 0.20128709],
       [0.18443804, 0.23529412, 0.08333333, 0.30534625, 0.41687522],
       [0.31412104, 0.35294118, 0.33333333, 0.40222461, 0.28816589],
       [0.19596542, 0.23529412, 0.08333333, 0.532831  , 0.2634966 ],
       [0.24495677, 0.35294118, 0.91666667, 0.38464299, 0.48623525]])

# 정규화된 범죄 건수가 들어있는 새로운 데이터 프레임 만들기
crime_anal_norm = pd.DataFrame(x_scaled, columns = cols, index = crime_anal.index)
crime_anal_norm.head()
        강간	강도	살인	절도	폭력
구별					
강남구	1.000000	0.941176	0.916667	1.000000	1.000000
강동구	0.155620	0.058824	0.166667	0.467528	0.437969
강북구	0.146974	0.529412	0.416667	0.133118	0.415445
관악구	0.628242	0.411765	0.583333	0.589523	0.647479
광진구	0.397695	0.529412	0.166667	0.704342	0.406864

# 검거율 합쳐주기 
col2 = ['강간검거율', '강도검거율', '살인검거율', '절도검거율', '폭력검거율']
crime_anal_norm[col2] = crime_anal[col2]
```

## CCTV, 인구수 데이터까지 포함하기

```python
result_CCTV = pd.read_csv('./data/pop_cctv_raw.csv', encoding='utf-8', index_col='구별')

crime_anal_norm[['인구수', 'CCTV']] = result_CCTV[['인구수', '소계']]
```

## 범죄 발생에 대한 합 구하기

- 어떠한 구에 어떤 범죄가 많이 일어나느냐 → 정규화를 사용해서 구했다.
- 어떠한 구에 범죄 자체가 많이 일어나느냐 → 범죄들의 합으로 구할 수 있다.

```python
col = ['강간', '강도', '살인', '절도', '폭력']
# 정규화된 범죄 발생 건수를 합하여 범죄 컬럼 구하기 
crime_anal_norm['범죄'] = np.sum(crime_anal_norm[col].axis =1)
```

## 검거율에 대한 합 구하기

```python
col = ['강간검거율', '강도검거율', '살인검거율', '절도검거율', '폭력검거율']
crime_anal_norm['검거'] = np.sum(crime_anal_norm[col], axis = 1)
```